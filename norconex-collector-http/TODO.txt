The following are either things to be done, or ideas to consider:

- Enhance <maxDepth> to allow specifying max depth from a URL pattern.
  E.g., one-level deep from any URL matching xyz*.

- Have tagger(s) that will store booleaen flags if a condition is met.
  E.g., Does the content of tag <xyz> contains this term?

- Form authentication: add ability to first parse a login page, look for 
  login form, and pass all its input field to the login request.

- Have ability to download and install committers from command line?

- Have ability to do POST instead of GET for matching URLs.

- Because the QueuePipeline can be invoked from anywhere, make the 
  "inscope" check part of it instead of always forcing to check first.

- Have an option that sends committer deletion requests at every rejection
  (once per session per URLs).

- Support custom arguments to the PhantomJS script file.
  https://github.com/Norconex/collector-http/issues/505

- Support HTTP v2.  Example site: https://www.eetimes.com/rss_simple.asp

- Add external site depth?

- Create parent project to set managed dependencies?

- Use safe filename methods for saving sitemap data (otherwise fails when 
  containing spaces or others).

- Add one of these as JVM arg: 
      -XX:ExitOnOutOfMemoryError
      -XX:CrashOnOutOfMemoryError 

- Add to FAQ: how to extract few different HTML tag scenarios and maybe 
  add a tagger that helps with that.

- Maybe rename REJECTED_UNMODIFIED|PREMATURE to IGNORED_* ? 

- Should HttpClientProxy (for PhantomJS) reuse the same http methods?
  (e.g., POST should be POST).  Also check that https sites can be handled 
  better.

- Have a new <inScopeStrategy class="..."> with the default implementation
  being [Generic]URLCrawlScopeStrategy, taking the stayOn* and implementing 
  an IURLCrawlScope interface.

- Investigate why defining tags (e.g. <httpClientFactory> under the collector
  root tag (as opposed to under crawler tag) does not produce validation error.

- Image cache gives issues with multiple crawlers using the same path, fix this.

- Make it possible to merge two or more docs.

- Maybe add "stayOnDomainMaxSub" for how many subdomain level to support?
  (default would be zero).

- Make default directory for logs and progress the workdir.

- Fix redirects used to authenticate or create session but otherwise return
  to original URL which then can be crawled.

- Change sample config to have just "$workdir" for logs and progress dirs.

- Change sample config to have crawlerDefaults empty instead.

- Remove <dependencyManagement> which is mainly valuable when used with
  parent projects (which we are not here).  Use regular <dependencies instead.
  Do this for all projects.

- Use Norconex Commons Lang Proxy class wherever proxies are used.

- Use Apache HttpClient-Win for Windows Auth support like done with
  Azure Search Committer.

- Have the config reference.xml put all settings in a crawler instead
  of crawling defaults (FS Collector as well).

- Ship with importer.[sh|bat] files (applies to FS Collector as well).

- Have a way to store all data for easy recrawl.

- Reorganize Basic+Execution tests to combine servlet test cases with their 
  test classes.

- On GenericLinkExtractor, offer a flag to convert \ to /.  Cannot be done
  at normalization time since relative URLs will not be converted to full
  URLs properly without that.

- Consider deprecating TikaLinkExctractor, or at least update it to support
  relative base hrefs (share some code with GenericLinkExtractor).

- Offer a separate process to run on the crawl store and produce some kind of 
  flat file with referring URL counts so people can decide to use this
  to create some form of relevancy in their search engine (e.g. using 
  inboud links as a popularity indicator).

- Have a global settings for making Properties (HttpMetadata, 
  ImporterMetadata, etc) case sensitive or not.

- To consider: when re-processing orphans, check the depth in case the 
  max depth changed.

- Offer a flag to merge frame/iframe elements on HTML pages as opposed to
  crawl them as separate documents.

- Add a command-line flag that saves the constructed config back to disk 
  for easy sharing (XML-write).

- Carry a flag that tells if we are running a full crawl, or an incremental one.
  Some operations may be relevant only on incremental runs.

- Add total execution time, periodic time elapsed and estimated remaining.

- Store somewhere in metadata (and maybe carry in code?) whether a 
  document is new or modified.

- When issuing a STOP, add an option to specify which crawler to stop,
  and let others run.

- Add a startup flag that will generate a batch/sh script for the user which 
  abstracts the call to the config file 
  (e.g., abcCollector.sh start|stop|resume)

- Ability to crawl up to a given size.  Absolute number or percentage of 
  disk capacity? Shall we tie that to checking for remaining space? 
  We could issue warnings and/or stop when threshold is reached to 
  prevent crashing due to lack of space.

- Introduce "Add-ons" like social media add-ons to crawl social media sites.

- Have an interface for how to optionally store downloaded files
  (i.e., location, directory structure, file naming).  This could allow
  usage of the collector to clone a site.  Should the DocumentFetcher do it 
  instead?

- Have a crawler event listener that generated a tree-like graph of all URLs
  (i.e. a kind of sitemap).
  
- To consider: Interface for how to save documents whey they are kept.
  Same with default committer queue location.
  File system is used for both now, but could be others like MongoDB?
  Same as previous item?   

- Add a command prompt action to flush the committer queue. This can be useful
  if the collector crashed for some reason, while there were files left in
  the committer queue.  Shall this be done on an AbstractCommitter class
  so all future collectors automatically has that?

- Add support for having different HTTPContext for each call and/or each
  sites.
  
- Provide some duplicate content detection mechanism.  Maybe using existing 
  checksums, or create a distinction between a "modified" checksum (to find out
  if a document was modified) and a "duplicate" checksum (to find out if 
  documents are identical).

- Add the ability to control how many successive crawls it has to go through
  before deleting a document because it was not found (404).

- Have configurable the level of verbosity desired when logging exceptions.
  The options could be:
     - type: none|all|first|last
     - stacktrace: false|true

- Rotate/break log files when too big.

- If a URL filter rule was changed and a document is now rejected (never 
  processed), it will not be deleted (since it did not get a 404/NOT_FOUND).
  Maybe check if rejected URL in URLProcessor are in cache and send deletion 
  if so.

- Integrate with distributed computing frameworks such as Hadoop.

- Test that IPV6 is supported (as domain names).

- Offer option to have crawling rate adjust itself if a site is slow
  (in case it is the crawler hammering it).  Probably a change or new delay
  implementation... this means total download time (both for HEAD and GET) 
  should be added as document properties (not a bad idea to do regardless).

- Add option to skip certain http response code (like 500).  Those docs 
  that should not be added nor deleted because they are in a temporary bad 
  state.  Is it really useful?

- Add a GUI application to help manage collectors and report useful info.

- Deal with <a rel="noreferrer" ... ??

MAYBE:
=======
- Start offering per-site options?  Like crawl interval and more?
  (can be achieve with defining different crawlers now).
